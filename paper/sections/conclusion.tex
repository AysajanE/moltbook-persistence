\section{Conclusion}
\label{sec:conclusion}
This paper measured conversational persistence on Moltbook during its first week
of public operation using a two-part definition: direct-reply incidence and
conditional reply speed. In this snapshot, the core pattern is
low-incidence/very-fast-conditional response: only 9.60\% of at-risk comments
receive a direct reply, but conditional reply times are concentrated in seconds
(\(t_{50}=4.55\) seconds, \(t_{90}=50.05\) seconds,
\(t_{95}=132.23\) seconds). Most comments receive no direct reply, reciprocal
interaction is uncommon, and thread geometry is shallow and root-heavy.

Model-to-observable validation shows tight calibration for reply incidence
overall and across key strata, while the same model overpredicts non-root
branching and deep-tail depth probabilities. A one-parent-per-thread robustness
check lowers pooled incidence but leaves conditional median speed nearly
unchanged, indicating that the central limitation is whether replies occur, not
how fast they arrive once they do. The kernel half-life diagnostic is
informative as a secondary exponential-equivalent timescale readout but is not
the primary persistence metric.

One hypothesis is an approximately 4-hour heartbeat schedule.
Because per-account heartbeat events are unobserved in the archive, we tested
only aggregate signatures. Event-time Rayleigh phase concentration is small
(\(r=0.0308\)), far below the estimated 80\%-power detectability threshold
(\(\kappa^\star=0.2\)). Consistent with that effect size, AR(1)-calibrated PSD
tests do not detect a strong 4-hour spectral line. This is an
effect-size-versus-detectability result, consistent with weak or dephased
periodic routines.

For context, a contemporaneous Reddit corpus analyzed with the same estimators
shows substantially deeper threads, higher direct-reply incidence, and
hour-scale persistence diagnostics. A coarse overlap-restricted matching
exercise points in the same direction but remains observational and non-causal.
The main next step is longitudinal measurement with richer exposure controls
(visibility, ranking, notifications) and richer agent-level models, enabling
sharper tests of lever-based intervention hypotheses such as memory aids,
thread summarization, and explicit re-entry prompts, including trade-offs across
incidence, conditional speed, and thread depth. In this first-week snapshot,
the evidence is consistent with the hypothesis that early agent social
platforms are more effective at initiating interactions than sustaining
multi-turn conversations without additional coordination scaffolds. These
design implications should be interpreted as model-consistent decision-support
hypotheses, not as experimentally validated causal intervention effects.
