\section{Data}
\label{sec:data}

We analyze the Moltbook Observatory Archive as our primary source for agent-driven conversations,
and we include a run-scoped curated Reddit corpus for full-scale platform-side baseline
estimation.

\subsection{Moltbook Observatory Archive}
\label{sec:data:moltbook}

Our primary dataset is the Moltbook Observatory Archive \citep{simulamet2026observatoryarchive}, a publicly available snapshot hosted on Hugging Face. The archive comprises multiple relational tables capturing platform activity from January 28 to February 4, 2026---the first week following Moltbook's public launch.

\subsubsection{Acquisition, Versioning, and Backfill}

We download and export the archive tables using our repository script (\texttt{scripts/download\_moltbook\_observatory\_archive.py}), storing immutable exports under \texttt{data/raw/} along with an export manifest (download timestamp, dataset identifier, and export format). Because the archive is updated via incremental exports and may backfill recent days, we treat \texttt{dump\_date} (when present) as a snapshot identifier. When constructing a canonical ``latest state'' view for analysis, we deduplicate by primary key (e.g., \texttt{comments.id}) and keep the record from the most recent available \texttt{dump\_date}.

\subsubsection{Dataset Structure}

The archive contains six tables, summarized in \cref{tab:dataset}.

\begin{table}[t]
\centering
\caption{Moltbook Observatory Archive structure in curated snapshot
\texttt{snapshot\_20260204-234429Z}.}
\label{tab:dataset}
\small
\begin{tabular}{@{}llrl@{}}
\toprule
\textbf{Table} & \textbf{Description} & \textbf{Rows} & \textbf{Key Fields} \\
\midrule
\texttt{agents} & Agent profiles and metadata & 25,597 & \texttt{id}, \texttt{karma}, \texttt{follower\_count} \\
\texttt{posts} & Root posts with scores & 119,677 & \texttt{id}, \texttt{agent\_id}, \texttt{submolt}, \texttt{created\_at\_utc} \\
\texttt{comments} & Comments with parent links & 226,173 & \texttt{id}, \texttt{post\_id}, \texttt{parent\_id}, \texttt{created\_at\_utc} \\
\texttt{submolts} & Community metadata & 3,678 & \texttt{name}, \texttt{subscriber\_count} \\
\texttt{snapshots} & Periodic observatory metrics & 114 & \texttt{timestamp}, \texttt{total\_agents}, \texttt{active\_agents\_24h} \\
\texttt{word\_frequency} & Hourly word counts & 15,346 & \texttt{word}, \texttt{hour}, \texttt{count} \\
\bottomrule
\end{tabular}
\end{table}

The \texttt{comments} table is central to our analysis. Each row contains:
\begin{itemize}
    \item \texttt{id}: unique comment identifier,
    \item \texttt{post\_id}: foreign key to the parent post,
    \item \texttt{agent\_id}: author identifier,
    \item \texttt{parent\_id}: identifier of the parent comment (null for direct replies to the post),
    \item \texttt{created\_at}: ISO 8601 timestamp,
    \item \texttt{score}: upvote count at fetch time.
\end{itemize}

The \texttt{parent\_id} field enables reconstruction of full comment trees, which is essential for computing depth, branching factors, and identifying reply chains.

\subsubsection{Preprocessing Pipeline}

We apply the following preprocessing steps:

\paragraph{Schema discovery and column retention.} We ingest all columns in each table and generate a schema manifest (column names and types) to guard against upstream schema changes. Where field names differ across sources or versions, we map them via an explicit schema dictionary.

\paragraph{Tree reconstruction.} For each post, we reconstruct the comment tree by linking comments via \texttt{parent\_id}. Comments with null \texttt{parent\_id} are direct children of the root post.

\paragraph{Deduplication and integrity checks.} Before reconstruction, we use canonical
``latest'' tables deduplicated by primary key using maximum \texttt{dump\_date}. In this
snapshot, canonical comments contain 223,317 unique comment IDs (from 226,173 raw comment
rows), and referential integrity checks pass: every \texttt{comments.post\_id} exists in
\texttt{posts.id}, every non-null \texttt{comments.parent\_id} exists in \texttt{comments.id},
and no negative parent or post lags are observed.

\paragraph{Timestamp normalization.} We parse all timestamps to UTC and retain raw timestamp strings for audit. For modeling, we convert times to numeric representations (e.g., Unix epoch seconds) and shift each thread so that its root post occurs at $t = 0$. This normalization facilitates aggregation across threads with different start times.

\paragraph{Agent resolution.} We join comments with the \texttt{agents} table to obtain author metadata (karma, follower count, and account age where available). If some comment authors are missing from the \texttt{agents} table, we retain these comments and handle missing covariates explicitly (e.g., missingness indicators, imputation, and sensitivity analyses).

\paragraph{Submolt categorization.} We assign each submolt to one of six labels
(\emph{Builder/Technical}, \emph{Philosophy/Meta}, \emph{Social/Casual},
\emph{Creative}, \emph{Spam/Low-Signal}, \emph{Other}) via a deterministic
keyword-based mapping in \texttt{analysis/06\_moltbook\_only\_analysis.py}. This avoids
manual post hoc relabeling and keeps category assignments reproducible across runs.

\paragraph{Coverage caveat.} Event-time analysis uses \texttt{created\_at\_utc}. We detect a
single 41.72-hour gap in comment timestamps (from 2026-01-31 10:37:53Z to
2026-02-02 04:20:50Z). As a result, periodicity analyses are computed on contiguous segments
rather than assuming complete continuous coverage.

\subsubsection{Descriptive Statistics}

\Cref{tab:descriptive} presents summary statistics for the processed dataset.

\begin{table}[t]
\centering
\caption{Descriptive statistics for processed Moltbook threads
(\(N=34{,}730\) threads with at least one comment).}
\label{tab:descriptive}
\small
\begin{tabular}{@{}lrrrrr@{}}
\toprule
\textbf{Metric} & \textbf{Mean} & \textbf{Median} & \textbf{Std} & \textbf{Min} & \textbf{Max} \\
\midrule
Comments per post & 6.43 & 5.00 & 7.04 & 1.00 & 846.00 \\
Max depth per thread & 1.38 & 1.00 & 0.49 & 1.00 & 5.00 \\
Thread duration (hours) & 0.06 & 0.04 & 0.21 & 0.00 & 20.60 \\
Unique agents per thread & 4.57 & 4.00 & 3.15 & 0.00 & 74.00 \\
Re-entry rate & 0.19 & 0.17 & 0.21 & 0.00 & 0.98 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Reddit Full-Scale Corpus}
\label{sec:data:reddit}

To contextualize Moltbook dynamics, we analyze Reddit run
\texttt{attempt\_scaled\_20260206-142651Z} from the curated pipeline.

\subsubsection{Subreddit Selection}

Run \texttt{attempt\_scaled\_20260206-142651Z} includes the following six subreddits:
\begin{itemize}
    \item \texttt{r/MachineLearning},
    \item \texttt{r/Python},
    \item \texttt{r/artificial},
    \item \texttt{r/datascience},
    \item \texttt{r/learnprogramming},
    \item \texttt{r/programming}.
\end{itemize}

\subsubsection{Matched Sample Construction}

The matched Moltbook--Reddit sample design is unchanged and is described in
\Cref{sec:methods:comparison}. In this revision, we report the Reddit side as an independent
full-scale baseline; matched inferential comparisons are future work.

\subsubsection{Data Collection}

Run \texttt{attempt\_scaled\_20260206-142651Z} contains 1,772 curated submissions and 9,878
curated comments, with 1,104 threads containing at least one comment. Comment timestamps span
2026-01-31T00:03:20Z to 2026-02-04T23:59:34Z (UTC). These run metadata are recorded in
\texttt{outputs/reddit\_only/attempt\_scaled\_20260206-142651Z/run\_manifest.json} and
\texttt{outputs/reddit\_only/attempt\_scaled\_20260206-142651Z/tables/analysis\_summary.json}.

\subsubsection{Validation and Curation Caveats}

Run-scoped validation passed (no failed checks) in
\texttt{outputs/reddit\_only/attempt\_scaled\_20260206-142651Z/validation\_results\_20260206T\_orchestrator.json}.
Two caveats from manifests are carried into downstream interpretation: (1) 1,570 comments were
dropped during curation because submission IDs were missing, and (2) the collection request log
contains 2 non-200/error responses.

\subsubsection{Use in This Manuscript}

For this revision, we compute Reddit-side geometry, survival, and periodicity metrics using the
same estimators as the Moltbook analysis and report them in \Cref{sec:results:reddit-full-scale}.
These results are descriptive and do not yet constitute matched cross-platform inference.

Ethical considerations for data use, privacy, and platform terms of service are discussed in \Cref{sec:limitations}.
