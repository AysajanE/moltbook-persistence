\section{Introduction}
\label{sec:introduction}

Recent advances in large language models (LLMs) have enabled a new class of autonomous artificial intelligence (AI) agents capable of sustained interaction with digital environments. One manifestation of this capability is \emph{Moltbook}, a social network launched in January 2026 that restricts posting privileges to AI agents while permitting human observation \citep{willison2026moltbook}. In this paper, an \emph{AI agent account} denotes an account whose posting and commenting actions are generated by an LLM-driven agent process rather than direct human operation. In the first archived week, the platform accumulated over 25{,}000 such agent accounts and 119{,}677 posts \citep{simulamet2026observatoryarchive}, creating a large, accessible dataset of agent-to-agent interaction.

This emergence of agent-populated social platforms raises fundamental questions about collective AI behavior. Can autonomous agents sustain the extended, multi-turn dialogues necessary for meaningful collaboration? How do architectural constraints---particularly context-window limitations and periodic activation schedules---shape the temporal dynamics of agent discourse? And what distinguishes agent-driven conversations from human-driven ones in structural and temporal terms?

\subsection{Motivation and Research Questions}

Early public commentary proposed that Moltbook agents may be better at
\emph{initiating} projects than \emph{sustaining} them and may follow roughly
4-hour check-in routines \citep{alexander2026afterweekend,willison2026moltbook}.
We use these statements as qualitative hypothesis motivation only, not as
factual evidence; empirical claims in this paper are derived from the archived
Observatory data. Because per-account heartbeat events are unobserved, we test
only aggregate periodic signatures, allowing for weak detectability under
dephasing/jitter. This framing is paired with finite context windows, which
motivate tests of rapid within-thread staleness.

Conversation persistence can be interpreted through canonical constructs in
service systems and stochastic-process modeling. Aggregate availability and
heartbeat schedules correspond to time-varying service capacity and cyclic
service regimes \citep{whitt2004efficiency,jouini2010online}. Staleness decay
corresponds to abandonment and impatience mechanisms in waiting systems
\citep{whitt2004efficiency,reed2012hazard,jouini2010online}. Reply incidence
corresponds to completion/throughput outcomes under constrained capacity, and
thread depth corresponds to branching stability versus subcriticality
\citep{harris1963theory}. This mapping motivates the measurement-target design
used in this study.

From an OR/MS decision-support perspective, the platform is a distributed
service system in which agents allocate limited attention across competing
thread ``jobs.'' The design question is operational: how scheduling or
resurfacing policies should prioritize pending threads to improve multi-step
coordination throughput under capacity limits, for example the share of threads
that reach depth \(\ge K\) and the incidence of re-entry. In this framing, our
contribution is a diagnostic decomposition: the
incidence margin identifies capacity-limited coordination failures, while the
conditional timing margin identifies latency-limited coordination failures.

Our objective is to develop and empirically validate a two-part decomposition
of conversational persistence in Moltbook's first week into (i) direct-reply
incidence and (ii) conditional reply latency, and to show that this
decomposition explains the observed shallow, root-heavy thread structure. In
scope are this decomposition, its structural consequences for depth, branching,
reciprocity, and re-entry, and heterogeneity analyses only when they sharpen
interpretation of the decomposition. Out of scope are dedicated thread-duration
inference, cross-platform baseline-comparison mechanics, and detailed periodicity machinery as
primary claims; thread duration is reported only as an ancillary
descriptive/contextual metric, and Reddit baseline context/periodicity are treated as
secondary contextual or supplementary analyses.

\paragraph{Identification scope.}
This paper reports observational measurement targets from archived first-week
data and does not identify causal intervention effects. Periodicity and
cross-platform baseline comparisons are contextual checks rather than primary
identification strategies. For heartbeat periodicity, ``not detected in this
snapshot'' denotes finite-window detectability limits rather than evidence of
absence.

\subsection{Hypotheses}
\label{sec:introduction:hypotheses}

Guided by the horizon-limited cascade framework (\Cref{sec:model}) and prior
qualitative commentary used for hypothesis motivation
\citep{willison2026moltbook, alexander2026afterweekend},
we test four hypotheses. H1a posits short exponential-equivalent kernel half-life
(diagnostic) values consistent with architectural staleness constraints. H1b
posits that heartbeat scheduling can
generate aggregate periodic structure near the hypothesized cadence
($\tau \approx 4$ hours) when check-ins are sufficiently synchronized; under
dephasing or jitter, aggregate detectability may be weak in finite samples.
H2 posits shallower, more root-concentrated Moltbook trees than human-platform
baselines, with lower reciprocity and conditioning-sensitive re-entry profiles; the
re-entry contrast is treated as conditioning-sensitive and may change direction
across baseline conditioning sets.
H3 posits topic-level moderation of persistence, including systematic differences
in kernel half-life diagnostic values and depth across submolts. H4 posits that
agent-level claim-status covariates are associated with variation in reply
incidence and conversational persistence.

We operationalize these hypotheses via the measurement targets defined in \Cref{sec:model}
and the estimators in \Cref{sec:methods}, and evaluate them in \Cref{sec:results}.
To make this mapping explicit, \Cref{tab:intro-roadmap} provides a compact
hypothesis-to-measurement-target roadmap with the corresponding empirical
readout sections.

\begin{table}[t]
\centering
\caption{Hypothesis roadmap: measurement targets, empirical readout location, and headline finding.}
\label{tab:intro-roadmap}
\begingroup
\setlength{\tabcolsep}{3pt}
\scriptsize
\renewcommand{\arraystretch}{1.15}
\begin{tabular}{@{}>{\raggedright\arraybackslash}p{0.09\linewidth}>{\raggedright\arraybackslash}p{0.33\linewidth}>{\raggedright\arraybackslash}p{0.22\linewidth}>{\raggedright\arraybackslash}p{0.30\linewidth}@{}}
\toprule
\textbf{Hyp.} & \textbf{Measurement target(s)} & \textbf{Section where tested} & \textbf{Key result} \\
\midrule
H1a & \(p_{\mathrm{obs}}\), conditional timing \(F_{T\mid\delta=1}\) (for example \(t_{50}, t_{90}\)), early-reply mass \(\Prob(\delta=1,T\le t)\), and kernel half-life diagnostic \(h\). & \Cref{sec:results:decomposition,sec:results:summary}. & Supported: low direct-reply incidence with very fast conditional timing (low-incidence/fast-conditional-response split). \\
H1b & Modulo-\(4\)-hour phase concentration (\(R\)), Rayleigh statistic (\(Z\)), and detectability target (\(\kappa^\star\)). & \Cref{sec:results:periodicity}. & Not supported as a strong aggregate 4-hour coherence claim in this snapshot; evidence is weak/dephased at the aggregate level. \\
H2 & Thread-geometry summaries (\(D_j,\hat{s}_{\mathrm{depth}},\bar c_k\)), reciprocity, re-entry \(\mathrm{RE}_j\), and Reddit baseline contrasts under the same estimators. & \Cref{sec:results:structure,sec:results:reddit-full-scale,sec:results:summary}. & Partially supported: Moltbook is shallow and root-concentrated; baseline context is deeper; reciprocity/re-entry contrast is conditioning-sensitive. \\
H3 & Submolt-stratified incidence/timing measurement targets, kernel half-life diagnostic differences, and topic-level depth-tail checks. & \Cref{sec:results:heterogeneity,sec:results:summary}. & Partially supported: topic moderation is clear for incidence/timing; depth moderation is present but deep-tail levels remain small. \\
H4 & Agent-covariate associations (claim status) in the two-part incidence/timing readout, with stratified diagnostics. & \Cref{sec:results:agent-covariates,sec:results:summary}. & Partially supported: claim-status associations are sizable descriptively, with dependence-limited model-based precision. \\
\bottomrule
\end{tabular}
\endgroup
\end{table}

\subsection{Preview of Findings}

In this first-week snapshot, Moltbook conversations are predominantly star-shaped,
with minute-scale kernel half-life diagnostic values, low direct-reply
incidence, and minimal
reciprocity. Periodicity effect size is weak (\(r=0.0308\), far below
\(\kappa^\star=0.2\)), and spectral diagnostics do not show a strong 4-hour line.
A run-scoped Reddit baseline shows materially longer
persistence and deeper threads. Taken together, these patterns are consistent
with a ``low-incidence/fast-conditional-response'' regime driven by architectural constraints
on agent attention.

\subsection{Approach and Contributions}

Aligned to this objective, the paper delivers three contributions. First, we
define a two-component persistence metric that separates reply incidence from
conditional reply timing. In OR terms, reply incidence corresponds to
completion probability/throughput, while conditional reply timing corresponds
to service time/latency, enabling diagnosis of whether coordination failure is
due to scarce engagement vs slow engagement, with the kernel half-life
diagnostic used only as a secondary timescale summary. Second, using the
Moltbook Observatory Archive
\citep{simulamet2026observatoryarchive}, we test whether low incidence and fast
conditional timing jointly account for shallow depth, root-heavy branching, low
reciprocity, and limited re-entry in first-week threads. Third, we quantify
topic- and agent-level heterogeneity only insofar as it clarifies which margin
of the decomposition drives those structural outcomes; thread-duration
summaries, Reddit baseline context, and detailed periodicity analyses are
retained as contextual or supplementary material rather than core deliverables.

\subsection{Paper Organization}

The remainder of this paper is organized as follows. \Cref{sec:background}
reviews stochastic-process foundations for interaction persistence, then network structure and agent-platform context.
\Cref{sec:model} presents the minimal model and primary measurement targets.
\Cref{sec:data} describes data construction and preprocessing.
\Cref{sec:methods} details estimation and empirical procedures.
\Cref{sec:results} reports empirical findings and model-consistency checks.
\Cref{sec:discussion} interprets implications, including short downstream design
considerations. \Cref{sec:limitations} addresses limitations and ethical
considerations. \Cref{sec:conclusion} concludes with key takeaways and future
work. Reproducibility and data-availability statements are provided in the
required manuscript statements, and supplementary material provides extended
derivations and diagnostics.
